{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7a60c6",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/external_images/testing_gif.gif\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6dd51",
   "metadata": {},
   "source": [
    "In this blog-post we are discussing the concept of transfer-learning and show an implementation using Python, using tensorflow. Namely, we are using the pre-trained model *MobileNetV2* and apply it on the *Oxford Flower 102* dataset, in order to build a flower classification model. Lastly we deploy it on an ios device to make live predictions, using the phone's camera. The Github Repository for this project can be found [here](https://github.com/paulmora-statworx/flower_detection).\n",
    "\n",
    "# Overview\n",
    "\n",
    "The concept of transfer-learning is best understood when applying the concept to real-life examples. If, for example, a person dedicately practices playing the piano for multiple years, this person is going to have an easier time picking up guitar playing, compared to a person who never played a musical instrument before. It does not mean, of course, that the person is going to be perfect at playing the guitar, but the yearlong finger practice of playing piano and the development of having a good ear in music makes it easier for the person to learn.\n",
    "\n",
    "It is important to note that these synergy effect were only possible because playing piano and playing the guitar were somewhat related. If the person who played piano for many years is now trying to learn to play American Football, it is rather less likely that this person will have an advantage. Hence, whether the previous training of concept A is beneficial for concept B, depends on whether concept A and B bear any similarities.\n",
    "\n",
    "The very same idea and conclusions apply to transfer-learning. If we build a strong classification model to detect apples, the odds are high, that we also could use the same model to classify oranges. It might not be as strong as a model which was initially trained on oranges, but it is definitely better than a poor orange classification model.\n",
    "\n",
    "Again, this would only work because apples and oranges bear some similarities (e.g. roundness). Though, if we would try to use the apple classification model and apply it to detect cars, we are likely getting a poor performance. \n",
    "\n",
    "The question might arise why we are not simply building a strong classification model using images of oranges. The reason for not doing could be multi-fold. It could be for example that we are lacking the computational power, the time, or even the amount of images needed to train a good-performing classification model. Especially the latter reason is a common problem within image detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10146047",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/ppt/starting_point.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee1b1a",
   "metadata": {},
   "source": [
    "The reason why a small amount of images is leading to a poor performing model is easily understood when thinking about what the training of a neural network is actually doing. When initializing a neural network, all weight parameters are initialized in a random manner. Through the training process these weight parameters are adjusting, using back-propagation based on gradient descent. If we do not have enough images of the object we would like to classify, the network is not going to have a sufficient amount of data in order to adjust the weights appropriately.\n",
    "\n",
    "Transfer-learning can then help in those situations. That is because of the workings of convolutional neural networks, which are the gold standard when working with image data. While many details of CNNs are not fully explored, we know that especially the lower levels of those models learn very general shapes and patterns of the images. In the final layers of the networks all these shapes and patterns are put together to make up for the final object. It is exactly that ability to detect shapes and patterns which are useful for not only the purposes the model was originally trained for, but for all kinds of use-cases.\n",
    "\n",
    "As mentioned before, the job of the last layers of the network is to put all the different shapes and patterns together and decides what this combination of many different aspects is most likely going to be. These last layers are, in contrast to the initial layers, very purpose specific. For that reason it is normally necessary to therefore stack one or several top layers on the pre-trained model and solely train those. The benefit of this method is that the ground-work was already done. One does not have to handcraft the jigsaw pieces anymore, but merely have to put them together.\n",
    "\n",
    "There are of course some restrictions when using this methodology. Firstly, it is necessary to pre-process the images in the very same way the pre-trained model did it. If, for example, the pre-trained model scaled all pixel values between 0 and 1 before training, we would have to do the very same with the new images we would like to classify, since the model would not know how to handle those otherwise. Though, we also have some freedom when using transfer-learning. If the pre-trained model originally was used to classify 50 different breeds of dogs, we could change that amount of categories, by replacing the top-layers with, for example, 30 different categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba0e0d",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/ppt/base_layer.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc0568",
   "metadata": {},
   "source": [
    "When it comes to training the stacked top-layers discussed before, it is important to know that we are not interested in training the pre-trained model, but only the top-layers. Jointly training the pre-trained model with the randomly initialized top-layers would result in too large gradient updates, and the pre-trained model would forget what it originally learned.\n",
    "\n",
    "The above does not mean that we cannot further re-fine the model. After training the stacked top-layers we can then unfreeze some higher layers of the pre-trained models and train the entire model again. It is crucial at this point to use a smaller learning rate, in order to note cause large gradient updates which would shake the foundation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd08878",
   "metadata": {},
   "source": [
    "# MobileNet V2\n",
    "\n",
    "After covering the general idea of transfer-learning, we are now slowly moving into the direction of implementing our own transfer-learning model. For that we first have to find a solid pre-trained model. As explained above, it is important for the cross-learning effects to happen that the pre-trained model is generalizable. Therefore, most of these pre-trained models are initially trained on very large image datasets with hundreds of categories. For that reason, many of the pre-trained models are trained using the [ImageNet database](https://www.image-net.org). This database offers 1000 categories and over one million training images.\n",
    "\n",
    "When choosing which pre-trained model to go for, we were influenced in our decision by the fact that we are interested to deploy our final model on a mobile device. That is because of the obvious limitations regarding storage space, processing speed and energy usage of mobile phones. Lucky for us, there is a pre-trained model for exactly that purpose, namely the [**MobileNet V2**]((https://arxiv.org/pdf/1704.04861.pdf)), which was developed by Google.\n",
    "\n",
    "## Model architecture\n",
    "\n",
    "In order to find out how this model is working under the hood, we take a look into the [original paper](https://arxiv.org/pdf/1801.04381v4.pdf). The graphic below shows the different layers which were used when initially training this model. Each line represents one layer, which is repeated *n* times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d476133",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/external_images/original_model.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794db269",
   "metadata": {},
   "source": [
    "[Source](https://arxiv.org/pdf/1801.04381v4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc444f4",
   "metadata": {},
   "source": [
    "The MobileNetV2 starts with a traditional convolution before starting to apply the so-called bottleneck operator. This operator is elaborated on in more detail in the following sections. The main idea of these bottleneck layers is that they try to maintain the same level of accuracy as original convolutional networks, while being computationally cheaper. The size of the feature maps being passed from one layer to the next, are denoted by the variable *c*, while the stride parameter is denoted as *s*. The parameter *t* describes the *expansion factor*, which is the amount by which the number of channels are expanded within the bottleneck methodology.\n",
    "\n",
    "As with most image classification models we notice how the number of channels gradually increase, while the image size decreases before collapsing the image into a 1x1 with k channels using average pooling and again a traditional convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae5d39",
   "metadata": {},
   "source": [
    "### Bottleneck Sequence\n",
    "\n",
    "The main difference between the workings of the MobileNetV2 and other image classification model architectures are the usage of these so-called bottlenecks. The original paper describes bottlenecks as a sequence of three things - a expansion layers, a depth wise convolution, and a projection layer. The following table, which was extracted from the original paper shows nicely how the input and output sizes change when applying the bottleneck sequence.\n",
    "\n",
    "One noteworthy aspect we gain from the table is that this approach is not using any pooling mechanism, and is altering the image's height and width solely using the stride parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd276a2",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/external_images/bottleneck_sizes.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291ece7",
   "metadata": {},
   "source": [
    "[Source](https://arxiv.org/pdf/1801.04381v4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a82f99",
   "metadata": {},
   "source": [
    "#### Expansion Layer\n",
    "\n",
    "As the name already suggests, what the expansion layer is doing is that it increases the number of feature maps of the output. This is done by applying multiple 1x1 kernel on the image and through that, increases the size of the channels without altering the height or width of the image. This can also be seen from the table above. The original paper describes this process as a form of *unzipping* the image into a larger workbench. By how much we are increasing the number feature maps is a user defined input. The original paper set the default value equal to six."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e5d7a",
   "metadata": {},
   "source": [
    "#### Depth wise convolution\n",
    "\n",
    "After increasing the number of channels through the expansion layer, we are then applying a so-called depth-wise convolution. Depth wise convolution is very similar to traditional convolution, with the only difference being that the result is not one pixel within a feature map, but multiple.\n",
    "\n",
    "To better understand that we quickly the workings of traditional convolution. Traditional convolution applies one (usually) squared kernel to an image, which then calculates the dot product with every feature map respectively and then calculates a weighted sum of all dot product results in return one single number. The resulting number of feature maps when applying a kernel to an image is equal to one.\n",
    "\n",
    "When applying depth wise convolution, we still apply the kernel to the image and calculate the dot product for every feature map. The difference is that we are then not summing the results of *all* feature maps together, but rather only sum the dot products for each feature map individually. This approach results into having the same amount of feature maps before and after applying the convolution. This is also visible by looking at the second row of the table above, in which it says that both the input and output are equal to $tk$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e78ffa",
   "metadata": {},
   "source": [
    "![](../reports/external_images/depthwise_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ab272",
   "metadata": {},
   "source": [
    "[Source](https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6ac6e",
   "metadata": {},
   "source": [
    "#### Projection layer\n",
    "\n",
    "Lastly we apply a so-called projection layer. What this layer is doing, is that it shrinks the number of feature maps. This is done by simply using again a 1x1 kernel, but this time not in order to increase the number of feature maps, but rather in order to decrease them. By how much the projection layer is shrinking the number of feature maps is a user-defined input, denoted as *c*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f92a34",
   "metadata": {},
   "source": [
    "![](../reports/external_images/pointwise_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadbd31",
   "metadata": {},
   "source": [
    "[Source](https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220f95a",
   "metadata": {},
   "source": [
    "#### Graphical Example\n",
    "\n",
    "To better understanding the combined workings of the bottleneck sequence, we are taking a look at an example. Let us assume that we are having an image with the sizes 128x128x16. When applying the expansion layer we are increasing the number of feature maps of the image. Using the default expansion factor of six, the number of channels increases to $16*6=96$, while not changing the width or height of the image.\n",
    "\n",
    "The second step would then be to apply the depth wise convolution. Given that the depth wise convolution is not changing the number of channels we do not have any change in the number of feature maps. Assuming a stride equal to 1, we also do not change the number of height or width.\n",
    "\n",
    "Lastly we are decreasing the number of feature channels again, using the projection layer. Herein we set the number of desired output channels equal to 24, which is therefore going to be the resulting number of output channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d08d4",
   "metadata": {},
   "source": [
    "![](../reports/ppt/filtering_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054b621",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "The question might be asked why we are doing all of this instead of simply using traditional convolution. The reason for this is mainly already answered by the invention of [MobileNetV1](https://arxiv.org/pdf/1704.04861.pdf). In contrast to MobileNetV2, V1 consists only of the Depth wise convolution plus the projection layer. This combination is by the original paper referred to as **Depth wise separable convolution**.\n",
    "\n",
    "In order to gain a more mathematical understanding of why the depth wise separable convolution is computationally beneficial is found when considering the number of computations both methods have to go through. A traditional convolutions have a computational cost of: \n",
    "\n",
    "$$D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F$$\n",
    "\n",
    "$$\\begin{align}\n",
    "D_K &= \\textrm{Kernel size} \\\\\n",
    "M &= \\textrm{Number of input channels} \\\\\n",
    "N &= \\textrm{Number of output channels} \\\\\n",
    "D_F &= \\textrm{Feature map size} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "In contrast to traditional convolution, depth wise convolution does not take the number of output channels into consideration, since it creates as many output channels as it has input channels by definition. Adjusting the number of output channels is then conducted by the projection layer. Both costs are then in the end summed together, resulting in the computational cost of the depth wise separable convolution: \n",
    "\n",
    "$$D_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F + M \\cdot N \\cdot D_F \\cdot D_F$$\n",
    "\n",
    "$$\\begin{align}\n",
    "D_K &= \\textrm{Kernel size} \\\\\n",
    "M &= \\textrm{Number of input channels} \\\\\n",
    "N &= \\textrm{Number of output channels} \\\\\n",
    "D_F &= \\textrm{Feature map size} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "When then calculating how many times the depth wise separable convolution is superior to the traditional convolution, we find the following:\n",
    "\n",
    "$$\\frac{D_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F + M \\cdot N \\cdot D_F \\cdot D_F}{D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F} = \\frac{1}{N} + \\frac{1}{D^2_K}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "D_K &= \\textrm{Kernel size} \\\\\n",
    "M &= \\textrm{Number of input channels} \\\\\n",
    "N &= \\textrm{Number of output channels} \\\\\n",
    "D_F &= \\textrm{Feature map size} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Assuming a Kernel size of 3, we then find that the convolution method of MobileNet is 8 to 9 times more efficient compared to traditional convolution.\n",
    "\n",
    "The computation gains are the main motivation of MobileNet overall. The difference between V2 and V1 are mostly the idea of zipping and unzipping the image within each bottleneck sequence, which further reduces the number of computational costs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739b6df",
   "metadata": {},
   "source": [
    "#### Side note: ReLU6\n",
    "\n",
    "Even though not particularly important is the fact the MobileNetV2 is not using a traditional ReLU function, but rather a so-called ReLU6 activation function. As the name probably already suggests, this caps all positive values at positive six, preventing the activations from becoming too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f2253",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/external_images/relu6.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ab9ba",
   "metadata": {},
   "source": [
    "# Oxford Flower 102\n",
    "\n",
    "In order to show the power of transfer-learning we chose the Oxford Flower 102 dataset, which can be found [here](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). This dataset contains, as the name suggests, 102 different categories of flowers in the United Kingdom. Each flower category contains between 40 and 258 images. Obviously, this amount of data is far too little in order to train a sophisticated neural network on it, which makes it a good testing example of transfer-learning.\n",
    "\n",
    "<div>\n",
    "<img src=\"../reports/external_images/flowers.jpeg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "[Source](https://www.researchgate.net/figure/Examples-of-images-in-the-Oxford-Flower-102-Dataset-Corresponding-categories-are-given_fig7_318204948)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b0b7d",
   "metadata": {},
   "source": [
    "# Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76939c7",
   "metadata": {},
   "source": [
    "The implementation of this transfer learning example was done in tensorflow. Choosing tensorflow over PyTorch did not have any particular reason, as both frameworks have a significant amount of content about transfer-learning. \n",
    "The repository for this project is found [here](https://github.com/paulmora-statworx/flower_detection). The final implementation of the trained model into an ios application is conducted following the tutorial from [tensorflow's repository](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef8f74",
   "metadata": {},
   "source": [
    "## Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbd18b",
   "metadata": {},
   "source": [
    "In contrast to many other blog-post which tries to show intuitive output after every code cell, this post is working a bit differently given the packaged code. Instead, this post shows all classes used in building the model individually and elaborates on its workings.\n",
    "\n",
    "In order to have a better understanding of how the different classes are interacting with each other, we start by showing the <code> src </code> folder for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eba806a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/\n",
      "├─config.json\n",
      "├─data_loader.py\n",
      "├─utils/\n",
      "│ ├─config.py\n",
      "│ └─args.py\n",
      "├─coreml_converter.py\n",
      "├─model.py\n",
      "├─trainer.py\n",
      "└─main.py\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "sd.seedir(\"../src\", style=\"lines\", exclude_folders=\"__pycache__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc825b70",
   "metadata": {},
   "source": [
    "As usually done, all major classes are constructed in their own file and then called and executed within <code> main.py </code>. All kinds of hyper-parameters for every class are stated within the <code> config.json </code> file and called by using an argparser function defined in the folder <code>utils</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8620503",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "The first step we have to do after downloading the images is to load them into Python. This step was less straight-forward than originally thought. This is because the Oxford Flower dataset has the interesting property of having substantially more test data than training data. This might be an interesting challenge for many, but for our use-case, in which we would like to end up building a strong classification model, we would rather have more trainings data.\n",
    "\n",
    "In order to self-set the train to test data ratio, we have to unpack all images and shuffle them ourselves. These steps are outlined in the methods <code> _loading_images_array </code> and <code> _load_labels </code>. Since labels and images are stored within two separate files, we have to make sure to correctly align and match image and label. This is done by sorting the image names in an ascending order before attaching the labels. The labels have to be one-hot encoded, in order to be properly used within the prediction algorithm.\n",
    "\n",
    "We decided to use the preprocessing class <code> ImageDataGenerator </code> from the preprocessing package from tensorflow. Using this preprocessing method allows us to easily apply the appropriate data augmentation settings for the images. One has to be aware that when using the pre-trained model MobileNetV2, one has to apply the related pre-processing function for that very model. This is necessary since the inputted images have to resemble the same kind of images which were used when training the model in the first place.\n",
    "\n",
    "Furthermore, we applied several data augmentation techniques to the trainings data. This is commonly done in situation in which we have only a limited amount of trainings data. In contrast to the trainings data, the validation and test data is not augmented in any way other than the necessary MobileNetV2 pre-processing. Classifying the flower category becomes a much harder challenge for the model when the image is heavily augmented. Since the validation data is not altered in any way, those examples are much easier for the model, which results in a better performance of on the validation data in contrast to the trainings data, a phenomena which rarely occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9e71bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %% Packages\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# %% Classes\n",
    "\n",
    "class OxfordFlower102DataLoader:\n",
    "    \"\"\"\n",
    "    This class loads the images and labels and embeds them into ImageDataGenerators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        (\n",
    "            self.train_generator,\n",
    "            self.val_generator,\n",
    "            self.test_generator,\n",
    "        ) = self.create_generators()\n",
    "\n",
    "    def create_generators(self):\n",
    "        \"\"\"\n",
    "        This method loads the labels and images, which are already split into train, test and validation.\n",
    "        Furthermore, we add an additional step to the preprocessing function, which is required for the pre-trained\n",
    "        model. Afterwards we create ImageGenerators from tensorflow for train, test and validation.\n",
    "        :return: ImageDataGenerator for training, validation and testing\n",
    "        \"\"\"\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self._image_and_labels()\n",
    "        train_augment_settings, test_augment_settings = self._add_preprocess_function()\n",
    "\n",
    "        # Data Augmentation setup initialization\n",
    "        train_data_gen = ImageDataGenerator(**train_augment_settings)\n",
    "        valid_data_gen = ImageDataGenerator(**test_augment_settings)\n",
    "        test_data_gen = ImageDataGenerator(**test_augment_settings)\n",
    "\n",
    "        # Setting up the generators\n",
    "        training_generator = train_data_gen.flow(\n",
    "            x=X_train, y=y_train, batch_size=self.config.data_loader.batch_size\n",
    "        )\n",
    "        validation_generator = valid_data_gen.flow(\n",
    "            x=X_val, y=y_val, batch_size=self.config.data_loader.batch_size\n",
    "        )\n",
    "        test_generator = test_data_gen.flow(\n",
    "            x=X_test, y=y_test, batch_size=self.config.data_loader.batch_size\n",
    "        )\n",
    "        return training_generator, validation_generator, test_generator\n",
    "\n",
    "    def _add_preprocess_function(self):\n",
    "        \"\"\"\n",
    "        This function adds the pre-processing function for the MobileNet_v2 to the settings dictionary.\n",
    "        The pre-processing function is needed since the base-model was trained using it.\n",
    "        :return: Dictionaries with multiple items of image augmentation\n",
    "        \"\"\"\n",
    "        train_augment_settings = self.config.data_loader.train_augmentation_settings\n",
    "        test_augment_settings = self.config.data_loader.test_augmentation_settings\n",
    "        train_augment_settings.update(\n",
    "            {\n",
    "                \"preprocessing_function\": tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "            }\n",
    "        )\n",
    "        test_augment_settings.update(\n",
    "            {\n",
    "                \"preprocessing_function\": tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "            }\n",
    "        )\n",
    "        return train_augment_settings, test_augment_settings\n",
    "\n",
    "    def _image_and_labels(self):\n",
    "        \"\"\"\n",
    "        This method loads labels and images and afterwards split them into training, validation and testing set\n",
    "        :return: Trainings, Validation and Testing Images and Labels\n",
    "        \"\"\"\n",
    "        y = self._load_labels()\n",
    "        X = self._loading_images_array()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            train_size=self.config.data_loader.train_size,\n",
    "            random_state=self.config.data_loader.random_state,\n",
    "            shuffle=True,\n",
    "            stratify=y,\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            train_size=self.config.data_loader.train_size,\n",
    "            random_state=self.config.data_loader.random_state,\n",
    "            shuffle=True,\n",
    "            stratify=y_train,\n",
    "        )\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def _load_labels(self):\n",
    "        \"\"\"\n",
    "        Loading the matlab file and one-hot encodes them.\n",
    "        :return: Numpy array of one-hot encoding labels\n",
    "        \"\"\"\n",
    "        imagelabels_file_path = \"./data/imagelabels.mat\"\n",
    "        image_labels = loadmat(imagelabels_file_path)[\"labels\"][0]\n",
    "        image_labels_2d = image_labels.reshape(-1, 1)\n",
    "\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        one_hot_labels = encoder.fit_transform(image_labels_2d)\n",
    "        return one_hot_labels\n",
    "\n",
    "    def _loading_images_array(self):\n",
    "        \"\"\"\n",
    "        Loading the flower images and resizes them into the appropriate size. Lastly we turn the images into a numpy array\n",
    "        :return: Numpy array of the images\n",
    "        \"\"\"\n",
    "        image_path = \"./data/jpg\"\n",
    "        image_file_names = os.listdir(image_path)\n",
    "        image_file_names.sort()\n",
    "        image_array_list = []\n",
    "        for image_file_name in image_file_names:\n",
    "            tf_image = tf.keras.preprocessing.image.load_img(\n",
    "                path=f\"{image_path}/{image_file_name}\",\n",
    "                grayscale=False,\n",
    "                target_size=(\n",
    "                    self.config.data_loader.target_size,\n",
    "                    self.config.data_loader.target_size,\n",
    "                ),\n",
    "            )\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(tf_image)\n",
    "            image_array_list.append(img_array)\n",
    "        return np.array(image_array_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b212e",
   "metadata": {},
   "source": [
    "Given that we have quite a large amount of flower categories (102), and the fact that these categories are not balanced, we have to make sure that we have the same proportion of each class within the trainings, validation and test data in order to have a stronger model and a more meaningful model evaluation. This balance is ensured by using the <code> stratify </code> argument within the train-test split from <code> sklearn </code>. The following image shows the result of using that parameter: We can see that we have same proportions within the train, test and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c00a6f",
   "metadata": {},
   "source": [
    "![](../reports/figures/relative_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53721632",
   "metadata": {},
   "source": [
    "In order to also have a better understanding what the pre-processing of the images is actually looking like, we show in the following nine example images of the trainings data. We see that all images are much darker than the original ones we saw before. That change of lighting comes from the MobileNetV2 pre-process function we applied. The image in the very middle of the lower matrix shows nicely the level of distortion we apply to the images. These augmentations of images are especially useful when having so little data as we do, since it artificially increases the pool of images we can train our model with. It is to be said though, that we are not applying these distortions on the test and validation data, since these heavy distortions are not occurring in the model's final application and would therefore should not be considered in the model's performance on real flower-images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5722afc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/figures/sample_images.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9e1b3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e6603",
   "metadata": {},
   "source": [
    "Now it is time to build our model. This is done in two steps. The first step loads the pre-trained model and freezes all parameters within it. We then stack a dense layer on top and solely train these weights. Furthermore, we add a dropout layer in order to prevent overfitting of the model.\n",
    "\n",
    "The second step, as already outlined in the explanation of transfer-learning, then describes the fine-tuning process of transfer-learning. Herein we unfreeze several of the top-layers of the pre-trained model and train them using a small learning rate in order to marginally adjust the pre-trained model in a beneficial direction.\n",
    "\n",
    "We are using RMSprop for compiling the model, as well as a learning rate of 1e-3 for the training within the first step, and a learning rate of 1e-4 for the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5288137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Packages\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# %% Classes\n",
    "\n",
    "\n",
    "class OxfordFlower102Model:\n",
    "    \"\"\"\n",
    "    This class is initializing the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.base_model = self.build_model()\n",
    "        tf.random.set_seed(self.config.model.random_seed)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        This method build the basic model. The basic model describes the pre-trained model plus a dense layer\n",
    "        on top which is individualized to the number of categories needed. The model is also compiled\n",
    "        :return: A compiled tensorflow model\n",
    "        \"\"\"\n",
    "        pre_trained_model = self.initialize_pre_trained_model()\n",
    "        top_model = self.create_top_layers()\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(pre_trained_model)\n",
    "        model.add(top_model)\n",
    "\n",
    "        model.compile(\n",
    "            loss=self.config.model.loss,\n",
    "            metrics=[self.config.model.metrics],\n",
    "            optimizer=tf.keras.optimizers.RMSprop(\n",
    "                learning_rate=self.config.model.learning_rate\n",
    "            ),\n",
    "        )\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def unfreeze_top_n_layers(self, model, ratio):\n",
    "        \"\"\"\n",
    "        This method unfreezes a certain number of layers of the pre-trained model and combines it subsequently with the\n",
    "        pre-trained top layer which was added within the 'create_top_layers' method and trained within the 'build_model'\n",
    "        class\n",
    "        :param model: Tensorflow model which was already fitted\n",
    "        :param ratio: Float of how many layers should not be trained of the entire model\n",
    "        :return: Compiled tensorflow model\n",
    "        \"\"\"\n",
    "        base_model = model.layers[0]\n",
    "        trained_top_model = model.layers[1]\n",
    "\n",
    "        base_model.trainable = True\n",
    "        number_of_all_layers = len(base_model.layers)\n",
    "        non_trained_layers = int(number_of_all_layers * ratio)\n",
    "        for layer in base_model.layers[:non_trained_layers]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        fine_tune_model = Sequential()\n",
    "        fine_tune_model.add(base_model)\n",
    "        fine_tune_model.add(trained_top_model)\n",
    "\n",
    "        adjusted_learning_rate = (\n",
    "            self.config.model.learning_rate / self.config.model.learning_rate_shrinker\n",
    "        )\n",
    "        fine_tune_model.compile(\n",
    "            loss=self.config.model.loss,\n",
    "            metrics=[self.config.model.metrics],\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=adjusted_learning_rate),\n",
    "        )\n",
    "        fine_tune_model.summary()\n",
    "        return fine_tune_model\n",
    "\n",
    "    def initialize_pre_trained_model(self):\n",
    "        \"\"\"\n",
    "        This method calls the pre-trained model. In this case we are loading the MobileNetV2\n",
    "        :return: Tensorflow model\n",
    "        \"\"\"\n",
    "        image_shape = (\n",
    "            self.config.data_loader.target_size,\n",
    "            self.config.data_loader.target_size,\n",
    "            3,\n",
    "        )\n",
    "        base_model = MobileNetV2(\n",
    "            input_shape=image_shape, include_top=False, pooling=\"avg\"\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "        return base_model\n",
    "\n",
    "    def create_top_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the tensorflow top-layer of a model\n",
    "        :return: Tensorflow Sequential model\n",
    "        \"\"\"\n",
    "        top_model = Sequential()\n",
    "        top_model.add(\n",
    "            Dense(self.config.model.number_of_categories, activation=\"softmax\")\n",
    "        )\n",
    "        top_model.add(Dropout(rate=self.config.model.dropout_rate))\n",
    "        return top_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b6193",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "As the last class, we define the trainings process. This class first triggers the training of the base-model, which comprises of the pre-trained model with the shallow top-layers on top for ten epochs. Afterwards, we call the unfreezing method of the model, which is defined within the model class, and continue training for ten more epochs. In order to stop any potential overfitting we are using Early-stopping. This method stops the model training after the validation accuracy leveled for a user-defined number of epochs (we chose three)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ee1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# %% Classes\n",
    "\n",
    "\n",
    "class OxfordFlower102Trainer:\n",
    "    \"\"\"\n",
    "    This class is training the base-model and fine-tunes the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data_generator, config):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.train_data_generator = data_generator.train_generator\n",
    "        self.val_data_generator = data_generator.val_generator\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "        self.val_loss = []\n",
    "        self.val_acc = []\n",
    "\n",
    "        self._init_callbacks()\n",
    "        print(\"Train the base Model!\")\n",
    "        self.train_model()\n",
    "        print(\"Fine tune the Model!\")\n",
    "        self.train_fine_tune()\n",
    "        self.save_model()\n",
    "\n",
    "    def _init_callbacks(self):\n",
    "        self.custom_callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                mode=\"max\",\n",
    "                patience=self.config.trainer.early_stopping_patience,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        This method is training the base_model\n",
    "        :return: /\n",
    "        \"\"\"\n",
    "        history = self.model.base_model.fit(\n",
    "            self.train_data_generator,\n",
    "            verbose=self.config.trainer.verbose_training,\n",
    "            epochs=self.config.trainer.number_of_base_epochs,\n",
    "            validation_data=self.val_data_generator,\n",
    "            callbacks=self.custom_callbacks,\n",
    "        )\n",
    "        self.append_model_data(history)\n",
    "\n",
    "    def train_fine_tune(self):\n",
    "        \"\"\"\n",
    "        This method is unfreezing some layers of the already trained model and re-trains the model\n",
    "        :return: /\n",
    "        \"\"\"\n",
    "        total_epochs = (\n",
    "            self.config.trainer.number_of_base_epochs\n",
    "            + self.config.trainer.number_of_fine_tune_epochs\n",
    "        )\n",
    "        self.fine_tune_model = self.model.unfreeze_top_n_layers(\n",
    "            self.model.base_model, self.config.trainer.percentage_of_frozen_layers\n",
    "        )\n",
    "\n",
    "        fine_tune_history = self.fine_tune_model.fit(\n",
    "            self.train_data_generator,\n",
    "            verbose=self.config.trainer.verbose_training,\n",
    "            initial_epoch=self.config.trainer.number_of_base_epochs,\n",
    "            epochs=total_epochs,\n",
    "            validation_data=self.val_data_generator,\n",
    "            callbacks=self.custom_callbacks,\n",
    "        )\n",
    "        self.append_model_data(fine_tune_history)\n",
    "        self.plot_history(\"fine_tune_model\")\n",
    "\n",
    "    def append_model_data(self, history):\n",
    "        \"\"\"\n",
    "        This method is\n",
    "        :param history: Tensorflow model history\n",
    "        :return: /\n",
    "        \"\"\"\n",
    "        self.loss.extend(history.history[\"loss\"])\n",
    "        self.val_loss.extend(history.history[\"val_loss\"])\n",
    "\n",
    "        self.acc.extend(history.history[\"accuracy\"])\n",
    "        self.val_acc.extend(history.history[\"val_accuracy\"])\n",
    "\n",
    "    def plot_history(self, title):\n",
    "        \"\"\"\n",
    "        This method is plotting the accuracy and loss of the plots\n",
    "        :param title: str - Used to save the png\n",
    "        :return: /\n",
    "        \"\"\"\n",
    "        fig, axs = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "        axs = axs.ravel()\n",
    "        axs[0].plot(self.loss, label=\"Training\")\n",
    "        axs[0].plot(self.val_loss, label=\"Validation\")\n",
    "        axs[0].set_title(\"Loss\")\n",
    "        axs[0].axvline(\n",
    "            x=(self.config.trainer.number_of_base_epochs - 1),\n",
    "            ymin=0,\n",
    "            ymax=1,\n",
    "            label=\"BaseEpochs\",\n",
    "            color=\"green\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        axs[0].legend()\n",
    "\n",
    "        axs[1].plot(self.acc, label=\"Training\")\n",
    "        axs[1].plot(self.val_acc, label=\"Validation\")\n",
    "        axs[1].set_title(\"Accuracy\")\n",
    "        axs[1].axvline(\n",
    "            x=(self.config.trainer.number_of_base_epochs - 1),\n",
    "            ymin=0,\n",
    "            ymax=1,\n",
    "            label=\"BaseEpochs\",\n",
    "            color=\"green\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.savefig(f\"./reports/figures/history_{title}.png\")\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Saving the fine-tuned model\n",
    "        :return: /\n",
    "        \"\"\"\n",
    "        path = \"./models/oxford_flower102_fine_tuning.h5\"\n",
    "        self.fine_tune_model.save(filepath=path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c38a7",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "Lastly we call all of the aforementioned classes within the <code> main.py </code> file and trigger them one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Packages\n",
    "\n",
    "from utils.args import get_args\n",
    "from utils.config import process_config\n",
    "from model import OxfordFlower102Model\n",
    "from data_loader import OxfordFlower102DataLoader\n",
    "from trainer import OxfordFlower102Trainer\n",
    "\n",
    "# %% Main Script\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = get_args()\n",
    "    config = process_config(args.config)\n",
    "\n",
    "    print(\"Creating the Data Generator!\")\n",
    "    data_loader = OxfordFlower102DataLoader(config)\n",
    "\n",
    "    print(\"Creating the Model!\")\n",
    "    model = OxfordFlower102Model(config)\n",
    "\n",
    "    print(\"Creating the Trainer!\")\n",
    "    trainer = OxfordFlower102Trainer(model, data_loader, config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b021c1",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "The plot below gives us some interesting insights into how the model training went. We see that the model reaches a relatively strong performance after only a small amount of training epochs, but then seems to starting leveling. After the fine-tuning kicks in, we then witness a significant drop in accuracy, which suggests that the learning rate was probably too high, triggering to large weight changes within the back-propagation of the network. Though, after a couple of epochs, the model is back on track, reaching performance levels which were not attainable earlier.\n",
    "\n",
    "Looking at the examples from the tensorflow [website](https://www.tensorflow.org/tutorials/images/transfer_learning), we did not spot any drop in accuracy to the extent that we encountered. Unsure whether that problem was only due to a potentially higher learning rate, we tried a range of learning rates, always encountering the same problem. We therefore suspect that the drop of the learning rate is likely going to be a result of having so little trainings data, compared to the example shown on the tensorflow website, which uses the well-known [cats vs. dogs dataset](https://www.kaggle.com/c/dogs-vs-cats).\n",
    "\n",
    "Overall we are happy with the model performance, which reaches an accuracy of **93.17%** on the unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc631e1",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../reports/figures/history_fine_tune_model.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b04d8",
   "metadata": {},
   "source": [
    "# Phone Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261417f",
   "metadata": {},
   "source": [
    "Finally we though it would be a nice to deploy the model on a mobile application, as that was also the motivation of choosing the MobileNetV2 network. In order to do so, one hast to convert the <code>h5</code> format the model is currently saved as, into a <code>tflite</code> file. Doing that compresses the model and brings it into the right format for the job. Furthermore, we have to sort and store all the labels into a text file and put them into the <code> Model </code> folder of the application parent folder.\n",
    "    \n",
    "This app folder is pulled from the official tensorflow repository, found [here](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67f9395b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/\n",
      "├─.DS_Store\n",
      "├─oxford_flower_102.tflite\n",
      "├─labels_flowers.txt\n",
      "├─mobilenet_quant_v1_224.tflite\n",
      "├─labels.txt\n",
      "└─.gitignore\n"
     ]
    }
   ],
   "source": [
    "sd.seedir(\"../app/model\", style=\"lines\", exclude_folders=\"__pycache__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Packages\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% Loading models and data\n",
    "\n",
    "# Model\n",
    "keras_path = \"./models/oxford_flower102_fine_tuning.h5\"\n",
    "keras_model = tf.keras.models.load_model(keras_path)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"./models/oxford_flower_102.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Labels\n",
    "labels_path = \"./data/cat_to_name.json\"\n",
    "with open(labels_path) as json_file:\n",
    "    labels_dict = json.load(json_file)\n",
    "sorted_labels_dict = sorted(labels_dict.items(), key=lambda x: int(x[0]))\n",
    "label_values = [x[1] for x in sorted_labels_dict]\n",
    "textfile = open(\"./models/labels_flowers.txt\", \"w\")\n",
    "for element in label_values:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fdf3d",
   "metadata": {},
   "source": [
    "After some adjustment in xcode, we can then deploy the app on any iOs device and use the camera for live prediction. The result of which can be seen on the gif below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822479f0",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<div>\n",
    "<img src=\"../reports/external_images/testing_gif.gif\" width=\"300\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower_detection",
   "language": "python",
   "name": "flower_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
